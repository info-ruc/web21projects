# 通过社交网络进行基于文本的抑郁症检测

**张腾甘 2017201988**

## 背景与动机

社交网络已经成为人们日常生活中非常重要的一部分，很多人会在贴吧、微博等社交网络上面去分享自己的情绪和感受。

抑郁症是全球一种常见病，估计共有3.5亿名患者。抑郁症不同于通常的情绪波动和对日常生活中挑战产生的短暂情绪反应。尤其是，长期的中度或重度抑郁症可能成为一个严重的疾患。患者可能会受极大影响，在工作中以及在学校和家中表现不佳。最严重时，抑郁症可引致自杀。每年自杀死亡人数估计高达100万人。

如果能够通过监测社交网络来识别出可能存在抑郁症的人群，并且对其进行针对性的干预，将具有十分重大的社会价值。

## 抑郁数据集构建

![negative_pipeline](./negative_pipeline.jpg)

- Step1. 从百度贴吧抓取原始数据集

  - 百度贴吧上具有大量活跃的用户，在抑郁吧和抑郁症吧里面有大量已经确诊抑郁症的患者在里面发言互动交流。通过Python爬虫爬取贴吧当中的帖子获得原始数据集，最终得到60w+条数据。

  - 抓取抑郁吧的帖子

    - 代码
      - TiebaCrawlerYiyu.py

    - 数据集

      - raw_data_yiyu.xlsx

      - raw_data_yiyu_new.xlsx
      - raw_data_yiyu_new2.xlsx

  - 抓取抑郁症吧的帖子

    - 代码
      - TiebaCrawlerYiyuzheng.py

    - 数据集

      - raw_data_yiyuzheng.xlsx

      - raw_data_yiyuzheng_new.xlsx
      - raw_data_yiyuzheng_new2.xlsx

- Step2. 从原始数据集中筛选确诊用户

  - 在得到原始数据之后，需要从中筛选出确认抑郁症的用户，由于纯人工进行筛选工作量太大，所以选取采用了如下流程进行数据的标注：①首先通过['确诊', '医院', '吃药', '轻度抑郁', '中度抑郁', '重度抑郁', '患病', '治疗', '发作']这样的关键词去筛选句子，将含有这些关键词的数据分离出来放到一个单独的文件里面，再人工对筛选出来的这样的具体进行核对，得到我们确诊用户发言的pattern；②使用sent2vec根据我们所爬下来的数据训练model，然后使用训练过的model去生成pattern的句向量，选取余弦相似度大于90%的句子并且不含有否定词的句子进行扩充。

  - 步骤1. 先通过关键词筛选，再人工核对

    - 筛选代码
      - SelectDataByKeywords.py
    - 筛选关键词
      - ['确诊', '医院', '吃药', '轻度抑郁', '中度抑郁', '重度抑郁', '患病', '治疗', '发作']

    - 代码筛选后存储名称

      - selected_data_yiyu.xlsx

      - selected_data_yiyuzheng.xlsx

    - 人工核对后存放确诊用户存放的文件名称
      - final_selected_data.xlsx

  - 步骤2. 通过句向量进行pattern的扩充，再通过余弦相似度扩充pattern

    - 先按照“，”和“。”对文本进行分割
      - 代码
        - pre_deal_data.py

    - 对单个句子进行分词，用空格分隔
      - 代码
        - word_segmentation.py

    - 使用sent2vec训练model

      - 训练语料
        - train_cut.txt

      - 训练方法
        - 在linux环境sent2vec里面运行如下代码
          - ./fasttext sent2vec -input train_data_cut.txt -output my_model -minCount 8 -dim 10 -epoch 9 -lr 0.2 -wordNgrams 2 -loss ns -neg 10 -thread 20 -t 0.000005 -dropoutK 4 -minCountLabel 20 -bucket 4000000 -maxVocabSize 750000 -numCheckPoints 10 

      - 模型名称
        - my_model.bin

    - 使用训练好的my_model.bin去生成每个句子的句向量

      - 代码
        - setence2vec.py

      - 存储名称
        - train_data_vector.xlsx

    - 根据已有的pattern进行扩充

      - 代码

        - calculate_similarity.py

      - 否定词

        - deny_list = ['没去', '想去', '咨询', '女友', '男友', '老公', '老婆', '吗', '女朋友', '女票',

          ​             '妈妈', '母亲', '爸爸', '父亲', '他', '她', '请问', '儿子', '女儿', '男朋友', '男票',

          ​             '？', '?', '女盆友', '男盆友', '孩子', '先生', '是不是', '老妈', '没有', '弟弟',

          ​             '怎么', '什么', '闺蜜']

      - 存储名称
        - critical_sentence_vector.xlsx

- Step3. 爬取确诊用户之前的所有留言

  - 通过上面的步骤，我们可以得到大量确认抑郁症的用户，对这些用户我们使用爬取其过去的发言来对其一段时间来的情况进行建模。
  - 输入数据集
    - final_selected_data.xlsx

  - 代码
    - TiebaHomeCrawler.py

  - 输出格式
    - 在tieba_data文件夹里面，文件名称为每个用户的user_id

## 非抑郁数据集构建

![positive_pipeline](./positive_pipeline.jpg)

- Step1. 从百度贴吧抓取原始数据集

  - 关于非抑郁的数据集在百度贴吧中有大量的来源

  - 抓取开心吧和快乐大本营吧的帖子

    - 代码
      - TiebaCrawlerPositive.py

    - 数据集

      - raw_data_kaixin.xlsx

      - raw_data_kuailedabenying.xlsx

- Step2. 爬取用户之前的所有留言

  - 输入数据集
    - raw_data_kaixin.xlsx
    - raw_data_kuailedabenying.xlsx

  - 代码
    - TiebaHomeCrawlerPositive.py

  - 输出格式
    - 在positive文件夹里面，文件名称为每个用户的user_id
    
      

## **数据集分析与对比**

| **Dataset**          | **Tieba Dataset(n)** | **Tieba Dataset(p)** | **Weibo Dataset(n)**<sup>[1]</sup> | **Weibo Dataset(p)**<sup>[1]</sup> | **Twitter Dataset(n)**<sup>[1]</sup> | **Twitter Dataset(p)**<sup>[1]</sup> |
| -------------------- | -------------------- | -------------------- | ---------------------------------- | ---------------------------------- | ------------------------------------ | ------------------------------------ |
| **User**             | 2,238                | 6,803                | 580                                | 580                                | 1,394                                | 1,394                                |
| **Post/Weibo/Tweet** | 18,899               | 9,624                | 45,461                             | 30,920                             | 290,886                              | 1,119,466                            |

​	•基于百度贴吧的抑郁症与非抑郁症用户数据集中用户的数量相比其他数据集<sup>[1]</sup>要多

​	•可能由于贴吧的用户黏性不强，发的帖子的数量相对要少一些

## **模型测试与结果分析**

| **model**          | **accuracy** | **recall** | **precision** | **F1** |
| ------------------ | ------------ | ---------- | ------------- | ------ |
| CNN                | 0.789        | 0.087      | 0.962         | 0.160  |
| LSTM               | 0.792        | 0.101      | 0.908         | 0.182  |
| Bidirectional LSTM | 0.789        | 0.104      | 0.947         | 0.187  |
| BERT               | 0.944        | 0.819      | 0.936         | 0.874  |

​	•在precision评估指标上，所有的模型都能够取得很好的效果，这能够保证对预测群体进行预测的准确性

​	•基于预训练模型的分类器能够在所有指标上取得较好的效果

## 总结

本课程项目的课题为《通过社交网络进行基于文本的抑郁症检测》。在数据部分，通过Python爬取了百度贴吧“抑郁吧”和“抑郁症吧”共计60w+条数据，并且采用了sent2vec的方法去生成每个句子的句向量，通过句向量相似度不断扩充能够判断确诊抑郁症的pattern实现确诊用户的批量标注，非确诊的数据则直接去“开心吧”等贴吧去爬取信息；在模型部分，尝试了CNN、LSTM、Bidirectional LSTM、C-LSTM和BERT，其中，为了能够提取用户过去一段时间中的时序特征，采用在BERT-Base-Chinese预训练语言模型的基础上进行Fine-tuning的方法，最终在测试集上达到了90%以上的分类准确率，取得了较为理想的结果。后期可通过多模态进一步提高模型准确率，比如加入对用户所发图片的特征提取，并且可以与其他社交平台所构建的数据集进行对比，分析不同平台的用户特异性。

## 参考文献

[1]SHEN T, JIA J, SHEN G, et al. Cross-Domain Depression Detection via Harvesting Social Media[C]//IJCAI, 2018: 1611–1617.

